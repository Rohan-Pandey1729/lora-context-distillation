INFO 11-19 22:25:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 11-19 22:25:04 [argparse_utils.py:79] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
[1;36m(APIServer pid=73517)[0;0m INFO 11-19 22:25:04 [api_server.py:1977] vLLM API server version 0.11.1
[1;36m(APIServer pid=73517)[0;0m INFO 11-19 22:25:04 [utils.py:253] non-default args: {'host': '0.0.0.0', 'model': 'Qwen/Qwen3-30B-A3B-Thinking-2507', 'max_model_len': 32768, 'quantization': 'fp8', 'enforce_eager': True, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.92, 'max_num_seqs': 64}
[1;36m(APIServer pid=73517)[0;0m INFO 11-19 22:25:13 [model.py:631] Resolved architecture: Qwen3MoeForCausalLM
[1;36m(APIServer pid=73517)[0;0m INFO 11-19 22:25:13 [model.py:1745] Using max model len 32768
[1;36m(APIServer pid=73517)[0;0m INFO 11-19 22:25:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=73517)[0;0m INFO 11-19 22:25:20 [vllm.py:500] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=78987)[0;0m INFO 11-19 22:25:27 [core.py:93] Initializing a V1 LLM engine (v0.11.1) with config: model='Qwen/Qwen3-30B-A3B-Thinking-2507', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-Thinking-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-Thinking-2507, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=78987)[0;0m WARNING 11-19 22:25:27 [multiproc_executor.py:869] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
ERROR 11-19 22:25:41 [multiproc_executor.py:743] WorkerProc failed to start.
ERROR 11-19 22:25:41 [multiproc_executor.py:743] Traceback (most recent call last):
ERROR 11-19 22:25:41 [multiproc_executor.py:743]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 715, in worker_main
ERROR 11-19 22:25:41 [multiproc_executor.py:743]     worker = WorkerProc(*args, **kwargs)
ERROR 11-19 22:25:41 [multiproc_executor.py:743]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 546, in __init__
ERROR 11-19 22:25:41 [multiproc_executor.py:743]     self.worker.init_device()
ERROR 11-19 22:25:41 [multiproc_executor.py:743]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/worker/worker_base.py", line 324, in init_device
ERROR 11-19 22:25:41 [multiproc_executor.py:743]     self.worker.init_device()  # type: ignore
ERROR 11-19 22:25:41 [multiproc_executor.py:743]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py", line 216, in init_device
ERROR 11-19 22:25:41 [multiproc_executor.py:743]     current_platform.set_device(self.device)
ERROR 11-19 22:25:41 [multiproc_executor.py:743]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/platforms/cuda.py", line 126, in set_device
ERROR 11-19 22:25:41 [multiproc_executor.py:743]     torch.cuda.set_device(device)
ERROR 11-19 22:25:41 [multiproc_executor.py:743]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
ERROR 11-19 22:25:41 [multiproc_executor.py:743]     torch._C._cuda_setDevice(device)
ERROR 11-19 22:25:41 [multiproc_executor.py:743] torch.AcceleratorError: CUDA error: uncorrectable ECC error encountered
ERROR 11-19 22:25:41 [multiproc_executor.py:743] Search for `cudaErrorECCUncorrectable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
ERROR 11-19 22:25:41 [multiproc_executor.py:743] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
ERROR 11-19 22:25:41 [multiproc_executor.py:743] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
ERROR 11-19 22:25:41 [multiproc_executor.py:743] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
ERROR 11-19 22:25:41 [multiproc_executor.py:743] 
INFO 11-19 22:25:41 [multiproc_executor.py:702] Parent process exited, terminating worker
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 102, in __init__
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]     self.model_executor = executor_class(vllm_config)
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 96, in __init__
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]     super().__init__(vllm_config)
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]     self._init_executor()
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 171, in _init_executor
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]   File "/gscratch/stf/rpande/lora-context-distillation/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py", line 653, in wait_for_ready
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842]     raise e from None
[1;36m(EngineCore_DP0 pid=78987)[0;0m ERROR 11-19 22:25:43 [core.py:842] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
