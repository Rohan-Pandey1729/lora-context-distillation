INFO 11-19 22:44:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 11-19 22:44:23 [argparse_utils.py:79] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
[1;36m(APIServer pid=79847)[0;0m INFO 11-19 22:44:23 [api_server.py:1977] vLLM API server version 0.11.1
[1;36m(APIServer pid=79847)[0;0m INFO 11-19 22:44:23 [utils.py:253] non-default args: {'host': '0.0.0.0', 'model': 'Qwen/Qwen3-30B-A3B-Thinking-2507', 'max_model_len': 32768, 'quantization': 'fp8', 'enforce_eager': True, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.92, 'max_num_seqs': 64}
[1;36m(APIServer pid=79847)[0;0m INFO 11-19 22:44:24 [model.py:631] Resolved architecture: Qwen3MoeForCausalLM
[1;36m(APIServer pid=79847)[0;0m INFO 11-19 22:44:24 [model.py:1745] Using max model len 32768
[1;36m(APIServer pid=79847)[0;0m INFO 11-19 22:44:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=79847)[0;0m INFO 11-19 22:44:33 [vllm.py:500] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=81158)[0;0m INFO 11-19 22:44:46 [core.py:93] Initializing a V1 LLM engine (v0.11.1) with config: model='Qwen/Qwen3-30B-A3B-Thinking-2507', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-Thinking-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-Thinking-2507, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=81158)[0;0m WARNING 11-19 22:44:46 [multiproc_executor.py:869] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 11-19 22:45:04 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:41211 backend=nccl
INFO 11-19 22:45:04 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:41211 backend=nccl
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 11-19 22:45:04 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 11-19 22:45:05 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 11-19 22:45:05 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 11-19 22:45:05 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 11-19 22:45:05 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(Worker_TP0 pid=81541)[0;0m INFO 11-19 22:45:06 [gpu_model_runner.py:3255] Starting to load model Qwen/Qwen3-30B-A3B-Thinking-2507...
[1;36m(Worker_TP1 pid=81542)[0;0m INFO 11-19 22:46:48 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP1 pid=81542)[0;0m INFO 11-19 22:46:48 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP0 pid=81541)[0;0m INFO 11-19 22:46:48 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP0 pid=81541)[0;0m INFO 11-19 22:46:48 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP1 pid=81542)[0;0m INFO 11-19 22:46:48 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP0 pid=81541)[0;0m INFO 11-19 22:46:48 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP1 pid=81542)[0;0m INFO 11-19 22:46:48 [fp8.py:180] Using Triton backend for FP8 MoE
[1;36m(Worker_TP0 pid=81541)[0;0m INFO 11-19 22:46:48 [fp8.py:180] Using Triton backend for FP8 MoE
