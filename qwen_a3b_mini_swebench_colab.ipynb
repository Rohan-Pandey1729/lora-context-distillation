{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1c6c75",
   "metadata": {},
   "source": [
    "\n",
    "# Qwen3 30B A3B - mini SWE-bench thinking-to-nonthinking distillation loop\n",
    "\n",
    "This Colab notebook does one full cycle:\n",
    "\n",
    "1. Run **mini-swe-agent** on `MariusHobbhahn/swe-bench-verified-mini` with the **thinking** model to produce patches.\n",
    "2. Build a distilled SFT dataset by pairing each input with the final patch and removing any thinking content.\n",
    "3. Fine-tune the **non-thinking** model with **rank-1 LoRA** using **Unsloth FastModel** and **merge after each step**.\n",
    "4. Create a **delta-interpolated** model by applying the weight difference between thinking and non-thinking bases onto the newly fine-tuned non-thinking model.\n",
    "5. Run the agent again with the delta model on the same mini set.\n",
    "6. Upload all artifacts to Hugging Face.\n",
    "7. Submit predictions to SWE-bench via `sb-cli` for the 50 mini IDs only.\n",
    "\n",
    "It is resumable and stateful across disconnects by pushing `state.json` and all intermediate artifacts to the Hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Setup\n",
    "# If running in Colab, uncomment the next line to ensure a clean environment\n",
    "# !pip -q install -U pip\n",
    "\n",
    "# Core deps\n",
    "!pip -q install -U \"transformers>=4.43.0\" accelerate bitsandbytes datasets peft trl einops safetensors fugashi ipadic\n",
    "# Unsloth with MoE FastModel support\n",
    "!pip -q install -U unsloth\n",
    "# Agentic harness + model router\n",
    "!pip -q install -U mini-swe-agent litellm pyyaml\n",
    "# vLLM OpenAI-compatible server\n",
    "!pip -q install -U \"vllm>=0.6.1\" uvicorn\n",
    "# Model merging\n",
    "!pip -q install -U mergekit\n",
    "# Hugging Face Hub utils\n",
    "!pip -q install -U huggingface_hub\n",
    "# SWE-bench remote evaluation CLI\n",
    "!pip -q install -U sb-cli\n",
    "\n",
    "import os, json, time, shutil, textwrap, subprocess, signal, sys, uuid, re, glob, math\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder, create_repo, login, upload_folder, snapshot_download\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c404db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Config - tokens, models, paths\n",
    "# Never print secrets\n",
    "HF_TOKEN = \"<REDACTED_HF_TOKEN>\"  #@param {type:\"string\"}\n",
    "HF_USERNAME = \"oof-baroomf\"                          #@param {type:\"string\"}\n",
    "SWEBENCH_API_KEY = \"<REDACTED_SWEBENCH_KEY>\"  #@param {type:\"string\"}\n",
    "\n",
    "# Base models\n",
    "THINKING_MODEL_ID = \"Qwen/Qwen3-30B-A3B-Thinking-2507\"   # thinking-only\n",
    "NONTHINK_MODEL_ID = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"   # non-thinking\n",
    "\n",
    "# Dataset\n",
    "SWEBENCH_MINI_ID = \"MariusHobbhahn/swe-bench-verified-mini\"\n",
    "SWEBENCH_SPLIT = \"test\"  # dataset is small; dev/test both acceptable, 'test' used when available\n",
    "\n",
    "# Agent sampling best practices for Qwen\n",
    "THINKING_GEN = dict(temperature=0.6, top_p=0.95, top_k=20, min_p=0.0, presence_penalty=1.0, max_tokens=32768)\n",
    "NONTHINK_GEN = dict(temperature=0.7, top_p=0.8,  top_k=20, min_p=0.0, presence_penalty=1.0, max_tokens=32768)\n",
    "\n",
    "# Training\n",
    "MAX_SEQ_LEN = 4096\n",
    "NUM_TRAIN_EPOCHS = 1.0\n",
    "GRAD_ACCUM = 2\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "LORA_RANK = 1\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.0\n",
    "MERGE_EVERY_STEP = True\n",
    "WARMUP_RATIO = 0.0\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Delta interpolation strength\n",
    "DELTA_ALPHA = 1.0  # new = FT_nonthink + alpha * (think_base - nonthink_base)\n",
    "\n",
    "# vLLM serve settings\n",
    "VLLM_PORT = 8000\n",
    "VLLM_GPU_UTIL = 0.9\n",
    "VLLM_MAX_MODEL_LEN = 393216  # be generous to avoid truncation\n",
    "\n",
    "# Persistent workdir\n",
    "ROOT = Path(\"/content\") if Path(\"/content\").exists() else Path.cwd()\n",
    "RUN_ROOT = ROOT / \"a3b_mini_runs\"\n",
    "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Remote persistence\n",
    "STATE_DATASET_REPO = f\"{HF_USERNAME}/a3b-mini-state\"\n",
    "PUSH_PUBLIC = True\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(RUN_ROOT / \"hf_home\")\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"SWEBENCH_API_KEY\"] = SWEBENCH_API_KEY\n",
    "\n",
    "# Login to HF non-interactively\n",
    "try:\n",
    "    login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "except Exception as e:\n",
    "    print(\"Hugging Face login note:\", type(e).__name__)\n",
    "    \n",
    "api = HfApi()\n",
    "try:\n",
    "    api.create_repo(STATE_DATASET_REPO, repo_type=\"dataset\", private=not PUSH_PUBLIC, exist_ok=True)\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "def _now():\n",
    "    return time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def _sh(cmd, env=None, check=True):\n",
    "    p = subprocess.run(cmd, shell=True, text=True, env=env or os.environ, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    if check and p.returncode != 0:\n",
    "        raise RuntimeError(p.stdout)\n",
    "    return p.stdout\n",
    "\n",
    "def _write_json(path, obj):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2))\n",
    "\n",
    "def _read_json(path, default=None):\n",
    "    try:\n",
    "        return json.loads(Path(path).read_text())\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _push_state(run_dir):\n",
    "    # push run_dir/state.json to dataset repo under state/{run_id}/\n",
    "    rel = f\"state/{run_dir.name}\"\n",
    "    upload_folder(folder_path=str(run_dir), repo_id=STATE_DATASET_REPO, repo_type=\"dataset\", path_in_repo=rel, token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Utilities: vLLM server, mini config, agent runner, sb-cli submit\n",
    "import yaml, tempfile, atexit\n",
    "\n",
    "_vllm_proc = None\n",
    "\n",
    "def start_vllm(model_id: str, port: int = VLLM_PORT, gpu_util: float = VLLM_GPU_UTIL, max_len: int = VLLM_MAX_MODEL_LEN):\n",
    "    global _vllm_proc\n",
    "    stop_vllm()\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", model_id,\n",
    "        \"--dtype\", \"auto\",\n",
    "        \"--port\", str(port),\n",
    "        \"--max-model-len\", str(max_len),\n",
    "        \"--gpu-memory-utilization\", str(gpu_util),\n",
    "        \"--disable-log-requests\",\n",
    "        \"--served-model-name\", \"qwen-local\",\n",
    "    ]\n",
    "    print(\"Starting vLLM:\", \" \".join(cmd))\n",
    "    _vllm_proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    # wait for server ready\n",
    "    started = False\n",
    "    start_t = time.time()\n",
    "    while time.time() - start_t < 180:\n",
    "        line = _vllm_proc.stdout.readline()\n",
    "        if not line:\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        if \"Uvicorn running on\" in line or \"Application startup complete\" in line:\n",
    "            started = True\n",
    "            break\n",
    "    if not started:\n",
    "        raise RuntimeError(\"vLLM did not start in time\")\n",
    "\n",
    "def stop_vllm():\n",
    "    global _vllm_proc\n",
    "    if _vllm_proc is not None:\n",
    "        try:\n",
    "            _vllm_proc.terminate()\n",
    "            _vllm_proc.wait(timeout=20)\n",
    "        except Exception:\n",
    "            try:\n",
    "                _vllm_proc.kill()\n",
    "            except Exception:\n",
    "                pass\n",
    "    _vllm_proc = None\n",
    "\n",
    "atexit.register(stop_vllm)\n",
    "\n",
    "def write_miniswea_config(path: Path, thinking: bool):\n",
    "    cfg = {\n",
    "        \"agent\": {\n",
    "            # Include a clear THOUGHT section but only one bash action per step\n",
    "            \"step_limit\": 48,\n",
    "            \"cost_limit\": 1000.0,\n",
    "            \"mode\": \"confirm\",\n",
    "            \"confirm_exit\": True,\n",
    "        },\n",
    "        \"environment\": {\n",
    "            \"environment_class\": \"local\",\n",
    "            \"env\": {\n",
    "                \"PAGER\": \"cat\",\n",
    "                \"MANPAGER\": \"cat\",\n",
    "                \"LESS\": \"-R\",\n",
    "                \"PIP_PROGRESS_BAR\": \"off\",\n",
    "                \"TQDM_DISABLE\": \"1\",\n",
    "            },\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"model_class\": \"litellm\",\n",
    "            \"model_name\": \"qwen-local\",\n",
    "            \"model_kwargs\": {\n",
    "                \"custom_llm_provider\": \"openai\",\n",
    "                \"api_base\": f\"http://127.0.0.1:{VLLM_PORT}/v1\",\n",
    "                \"api_key\": \"sk-local\",  # dummy, not used by vLLM\n",
    "                \"model\": \"qwen-local\",\n",
    "                \"drop_params\": True,  # ignore unsupported params\n",
    "            },\n",
    "        },\n",
    "        \"run\": {\n",
    "            # default prompts are fine for SWE-bench\n",
    "        },\n",
    "    }\n",
    "    # Qwen best practices\n",
    "    gen = THINKING_GEN if thinking else NONTHINK_GEN\n",
    "    cfg[\"model\"][\"model_kwargs\"].update(gen)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(yaml.safe_dump(cfg))\n",
    "    return path\n",
    "\n",
    "def run_mini_swebench(dataset_id: str, split: str, out_dir: Path, config_path: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Use the documented batch runner module\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"minisweagent.run.extra.swebench\",\n",
    "        dataset_id, \"--split\", split,\n",
    "        \"--config\", str(config_path),\n",
    "        \"--output-dir\", str(out_dir),\n",
    "    ]\n",
    "    print(\"Running mini-swe-agent:\", \" \".join(cmd))\n",
    "    out = _sh(\" \".join(cmd), check=False)\n",
    "    # preds.json should exist if any instance finished\n",
    "    preds = out_dir / \"preds.json\"\n",
    "    if preds.exists():\n",
    "        return json.loads(preds.read_text())\n",
    "    # make empty file to be safe\n",
    "    preds.write_text(json.dumps({}, indent=2))\n",
    "    return {}\n",
    "\n",
    "def collect_instance_ids(dataset_id: str, split: str):\n",
    "    ds = load_dataset(dataset_id, split=split)\n",
    "    # heuristically support different field names\n",
    "    key = \"instance_id\" if \"instance_id\" in ds.features else (\"id\" if \"id\" in ds.features else None)\n",
    "    if key is None:\n",
    "        ids = [row.get(\"instance_id\", row.get(\"id\", f\"idx_{i}\")) for i, row in enumerate(ds)]\n",
    "    else:\n",
    "        ids = [row[key] for row in ds]\n",
    "    return [str(x) for x in ids]\n",
    "\n",
    "def sbcli_submit(predictions_path: Path, subset: str, split: str, run_id: str, instance_ids: list[str]):\n",
    "    # sb-cli supports passing only some instance IDs\n",
    "    env = os.environ.copy()\n",
    "    env[\"SWEBENCH_API_KEY\"] = SWEBENCH_API_KEY\n",
    "    ids_arg = \",\".join(instance_ids[:5000])\n",
    "    cmd = [\n",
    "        \"sb-cli\", \"submit\", subset, split,\n",
    "        \"--predictions_path\", str(predictions_path),\n",
    "        \"--instance_ids\", ids_arg,\n",
    "        \"--run_id\", run_id,\n",
    "        \"--gen_report\", \"1\",\n",
    "    ]\n",
    "    print(\"sb-cli:\", \" \".join(cmd))\n",
    "    try:\n",
    "        out = _sh(\" \".join(cmd), env=env, check=False)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        return f\"sb-cli error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f56b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Data prep: from agent preds to SFT pairs\n",
    "def preds_to_sft_pairs(preds: dict, dataset_id: str, split: str):\n",
    "    # Load problems to map instance_id -> problem statement\n",
    "    ds = load_dataset(dataset_id, split=split)\n",
    "    # Build index\n",
    "    idx = {}\n",
    "    for row in ds:\n",
    "        iid = row.get(\"instance_id\", row.get(\"id\"))\n",
    "        task = row.get(\"problem_statement\", row.get(\"problem\", \"\"))\n",
    "        idx[str(iid)] = str(task)\n",
    "    # Pairs\n",
    "    records = []\n",
    "    for iid, payload in preds.items():\n",
    "        patch = payload.get(\"model_patch\", \"\")\n",
    "        if not patch:\n",
    "            continue\n",
    "        task = idx.get(str(iid), \"\")\n",
    "        # Strip any <think> blocks or THOUGHT markers if present\n",
    "        patch_clean = re.sub(r\"<think>.*?</think>\", \"\", patch, flags=re.DOTALL)\n",
    "        patch_clean = re.sub(r\"(?is)^.*?FINAL\\s*PATCH[:\\-]*\", \"\", patch_clean).strip()\n",
    "        records.append({\n",
    "            \"instance_id\": iid,\n",
    "            \"prompt\": task,\n",
    "            \"target\": patch_clean,\n",
    "        })\n",
    "    return records\n",
    "\n",
    "def save_sft_jsonl(pairs: list[dict], out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in pairs:\n",
    "            f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a1ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Rank-1 LoRA SFT with Unsloth.FastModel, merge every step\n",
    "from unsloth import FastModel\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "def train_rank1_lora_merge_each_step(base_model_id: str, sft_pairs_path: Path, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Load dataset into jsonlines -> HF Dataset\n",
    "    records = [json.loads(x) for x in Path(sft_pairs_path).read_text().splitlines() if x.strip()]\n",
    "    if not records:\n",
    "        raise RuntimeError(\"No SFT pairs found - cannot train\")\n",
    "    ds = Dataset.from_list([{\n",
    "        \"text\": f\"<|im_start|>system\\nYou are a helpful coding assistant.\\n<|im_end|>\\n\"\n",
    "                f\"<|im_start|>user\\n{r['prompt']}\\n<|im_end|>\\n\"\n",
    "                f\"<|im_start|>assistant\\n{r['target']}\\n<|im_end|>\\n\"\n",
    "    } for r in records])\n",
    "\n",
    "    # Load base with 4-bit for memory - no GGUF anywhere\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        base_model_id,\n",
    "        load_in_4bit=True,\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "    )\n",
    "    model = FastModel.get_peft_model(\n",
    "        model,\n",
    "        r=LORA_RANK,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=\"all-linear\",  # include MoE FFNs\n",
    "        use_dora=False,\n",
    "        bias=\"none\",\n",
    "        modules_to_save=None,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    def tokenize(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "        )\n",
    "    tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    from transformers import TrainingArguments\n",
    "    from trl import SFTTrainer\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(out_dir),\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        bf16=True,\n",
    "        logging_steps=1,\n",
    "        save_steps=0,\n",
    "        report_to=[],\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        dataloader_num_workers=2,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        max_grad_norm=1.0,\n",
    "    )\n",
    "\n",
    "    class MergeEveryStepCallback:\n",
    "        def __init__(self, model, save_dir: Path):\n",
    "            self.model = model\n",
    "            self.save_dir = save_dir\n",
    "            self.global_step = 0\n",
    "        def on_step_end(self, args, state, control, **kwargs):\n",
    "            self.global_step = state.global_step\n",
    "            if MERGE_EVERY_STEP:\n",
    "                # Merge LoRA into base weights and save a rolling checkpoint\n",
    "                try:\n",
    "                    self.model.merge_and_unload()\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        FastModel.merge_lora(self.model)  # fallback\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                step_dir = self.save_dir / f\"merged_step_{self.global_step:06d}\"\n",
    "                step_dir.mkdir(parents=True, exist_ok=True)\n",
    "                self.model.save_pretrained(step_dir)\n",
    "                tokenizer.save_pretrained(step_dir)\n",
    "                # Re-enable LoRA adapters for continued training\n",
    "                try:\n",
    "                    FastModel.enable_lora(self.model)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return control\n",
    "\n",
    "        # Hook names expected by TRL/Transformers\n",
    "        def on_train_end(self, args, state, control, **kwargs):\n",
    "            # Final merge\n",
    "            try:\n",
    "                self.model.merge_and_unload()\n",
    "            except Exception:\n",
    "                try:\n",
    "                    FastModel.merge_lora(self.model)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            final_dir = self.save_dir / \"merged_final\"\n",
    "            final_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self.model.save_pretrained(final_dir)\n",
    "            tokenizer.save_pretrained(final_dir)\n",
    "            return control\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=tokenized,\n",
    "        args=args,\n",
    "        dataset_text_field=None,\n",
    "        packing=False,\n",
    "        max_seq_length=MAX_SEQ_LEN,\n",
    "    )\n",
    "\n",
    "    merge_cb = MergeEveryStepCallback(model, out_dir)\n",
    "    trainer.add_callback(merge_cb)\n",
    "    trainer.train()\n",
    "\n",
    "    final_path = out_dir / \"merged_final\"\n",
    "    return final_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04462b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Delta interpolation with MergeKit\n",
    "def merge_delta_with_mergekit(nonthink_ft_dir: Path, think_base_id: str, nonthink_base_id: str, out_dir: Path, alpha: float = 1.0):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cfg = {\n",
    "        \"merge_method\": \"linear\",\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"slices\": [\n",
    "            {\"sources\": [\n",
    "                {\"model\": str(nonthink_ft_dir), \"layer_range\": [0, 1], \"weight\": 1.0},\n",
    "                {\"model\": think_base_id,       \"layer_range\": [0, 1], \"weight\": alpha},\n",
    "                {\"model\": nonthink_base_id,    \"layer_range\": [0, 1], \"weight\": -alpha},\n",
    "            ]}\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"normalize\": False\n",
    "        },\n",
    "    }\n",
    "    cfg_path = out_dir / \"merge.yml\"\n",
    "    cfg_path.write_text(yaml.safe_dump(cfg))\n",
    "    # run mergekit\n",
    "    cmd = f\"mergekit-yaml {cfg_path} {out_dir}\"\n",
    "    print(cmd)\n",
    "    out = _sh(cmd, check=False)\n",
    "    # Some versions output directly into out_dir - ensure model files exist\n",
    "    safes = list(out_dir.glob(\"**/*.safetensors\"))\n",
    "    if not safes:\n",
    "        raise RuntimeError(\"MergeKit did not produce weights\")\n",
    "    return out_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34050aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Orchestrator: run one cycle or resume\n",
    "def load_or_init_state(run_name: str):\n",
    "    run_dir = RUN_ROOT / run_name\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    state_path = run_dir / \"state.json\"\n",
    "    # Try pull from Hub first\n",
    "    try:\n",
    "        snapshot_download(repo_id=STATE_DATASET_REPO, repo_type=\"dataset\", local_dir=run_dir.parent, allow_patterns=f\"state/{run_name}/*\", token=HF_TOKEN)\n",
    "    except Exception:\n",
    "        pass\n",
    "    st = _read_json(state_path, default={\n",
    "        \"run_name\": run_name,\n",
    "        \"created_at\": _now(),\n",
    "        \"cycle\": 0,\n",
    "        \"steps\": {\n",
    "            \"thinking_done\": False,\n",
    "            \"distill_done\": False,\n",
    "            \"delta_done\": False,\n",
    "            \"eval2_done\": False,\n",
    "            \"uploads_done\": False,\n",
    "        },\n",
    "        \"models\": {\n",
    "            \"thinking_base\": THINKING_MODEL_ID,\n",
    "            \"nonthinking_base\": NONTHINK_MODEL_ID,\n",
    "            \"thinking_for_cycle\": THINKING_MODEL_ID,\n",
    "            \"nonthinking_for_cycle\": NONTHINK_MODEL_ID,\n",
    "            \"nonthinking_ft\": None,\n",
    "            \"delta_model\": None,\n",
    "        },\n",
    "        \"paths\": {},\n",
    "        \"reports\": {},\n",
    "        \"instance_ids\": [],\n",
    "    })\n",
    "    _write_json(state_path, st)\n",
    "    _push_state(run_dir)\n",
    "    return st, run_dir, state_path\n",
    "\n",
    "def save_state(st, state_path, run_dir):\n",
    "    _write_json(state_path, st)\n",
    "    _push_state(run_dir)\n",
    "    return st\n",
    "\n",
    "def run_one_cycle_or_resume(run_name: str = None):\n",
    "    run_name = run_name or f\"mini_a3b_{_now()}_{uuid.uuid4().hex[:6]}\"\n",
    "    st, run_dir, state_path = load_or_init_state(run_name)\n",
    "    cycle = int(st[\"cycle\"])\n",
    "\n",
    "    # 0) instance ids\n",
    "    if not st[\"instance_ids\"]:\n",
    "        ids = collect_instance_ids(SWEBENCH_MINI_ID, SWEBENCH_SPLIT)\n",
    "        st[\"instance_ids\"] = ids\n",
    "        save_state(st, state_path, run_dir)\n",
    "\n",
    "    # 1) Thinking pass with mini-swe-agent + vLLM\n",
    "    think_out_dir = run_dir / f\"cycle_{cycle:03d}\" / \"thinking_pass\"\n",
    "    preds_json = think_out_dir / \"preds.json\"\n",
    "    if not st[\"steps\"][\"thinking_done\"]:\n",
    "        stop_vLLM_on_error = True\n",
    "        try:\n",
    "            cfg_path = write_miniswea_config(run_dir / \"miniswea_thinking.yaml\", thinking=True)\n",
    "            start_vllm(st[\"models\"][\"thinking_for_cycle\"])\n",
    "            preds = run_mini_swebench(SWEBENCH_MINI_ID, SWEBENCH_SPLIT, think_out_dir, cfg_path)\n",
    "            preds_json.write_text(json.dumps(preds, indent=2))\n",
    "            st[\"paths\"][\"thinking_preds\"] = str(preds_json)\n",
    "            st[\"steps\"][\"thinking_done\"] = True\n",
    "            save_state(st, state_path, run_dir)\n",
    "        finally:\n",
    "            stop_vllm()\n",
    "\n",
    "    # 2) Build SFT pairs and train rank-1 LoRA on non-thinking model\n",
    "    sft_path = run_dir / f\"cycle_{cycle:03d}\" / \"sft_pairs.jsonl\"\n",
    "    ft_dir = run_dir / f\"cycle_{cycle:03d}\" / \"nonthink_ft\"\n",
    "    if not st[\"steps\"][\"distill_done\"]:\n",
    "        preds = _read_json(preds_json, default={})\n",
    "        pairs = preds_to_sft_pairs(preds, SWEBENCH_MINI_ID, SWEBENCH_SPLIT)\n",
    "        save_sft_jsonl(pairs, sft_path)\n",
    "        ft_merged_dir = train_rank1_lora_merge_each_step(st[\"models\"][\"nonthinking_for_cycle\"], sft_path, ft_dir)\n",
    "        st[\"models\"][\"nonthinking_ft\"] = str(ft_merged_dir)\n",
    "        st[\"models\"][\"nonthinking_for_cycle\"] = str(ft_merged_dir)  # use FT as next cycle's base\n",
    "        st[\"steps\"][\"distill_done\"] = True\n",
    "        save_state(st, state_path, run_dir)\n",
    "\n",
    "    # 3) Apply delta between base think and base nonthink onto new FT\n",
    "    delta_dir = run_dir / f\"cycle_{cycle:03d}\" / \"delta_model\"\n",
    "    if not st[\"steps\"][\"delta_done\"]:\n",
    "        merged_dir = Path(st[\"models\"][\"nonthinking_ft\"])\n",
    "        delta_model_dir = merge_delta_with_mergekit(\n",
    "            nonthink_ft_dir=merged_dir,\n",
    "            think_base_id=st[\"models\"][\"thinking_base\"],\n",
    "            nonthink_base_id=st[\"models\"][\"nonthinking_base\"],\n",
    "            out_dir=delta_dir,\n",
    "            alpha=DELTA_ALPHA,\n",
    "        )\n",
    "        st[\"models\"][\"delta_model\"] = str(delta_model_dir)\n",
    "        # the new \"thinking\" for next cycle becomes the delta model\n",
    "        st[\"models\"][\"thinking_for_cycle\"] = str(delta_model_dir)\n",
    "        st[\"steps\"][\"delta_done\"] = True\n",
    "        save_state(st, state_path, run_dir)\n",
    "\n",
    "    # 4) Evaluate delta model with agent again\n",
    "    eval2_dir = run_dir / f\"cycle_{cycle:03d}\" / \"delta_eval\"\n",
    "    preds2_json = eval2_dir / \"preds.json\"\n",
    "    if not st[\"steps\"][\"eval2_done\"]:\n",
    "        try:\n",
    "            cfg_path = write_miniswea_config(run_dir / \"miniswea_nonthinking.yaml\", thinking=False)\n",
    "            start_vllm(st[\"models\"][\"thinking_for_cycle\"])  # new delta model\n",
    "            preds2 = run_mini_swebench(SWEBENCH_MINI_ID, SWEBENCH_SPLIT, eval2_dir, cfg_path)\n",
    "            preds2_json.write_text(json.dumps(preds2, indent=2))\n",
    "            st[\"paths\"][\"delta_preds\"] = str(preds2_json)\n",
    "            st[\"steps\"][\"eval2_done\"] = True\n",
    "            save_state(st, state_path, run_dir)\n",
    "        finally:\n",
    "            stop_vllm()\n",
    "\n",
    "    # 5) Upload artifacts to Hub\n",
    "    if not st[\"steps\"][\"uploads_done\"]:\n",
    "        # Upload SFT, FT model, delta model, preds\n",
    "        pairs_repo = f\"{HF_USERNAME}/{run_name}-cycle{cycle:03d}-sft\"\n",
    "        ft_repo = f\"{HF_USERNAME}/{run_name}-cycle{cycle:03d}-nonthink-ft\"\n",
    "        delta_repo = f\"{HF_USERNAME}/{run_name}-cycle{cycle:03d}-delta\"\n",
    "        try:\n",
    "            api.create_repo(pairs_repo, repo_type=\"dataset\", private=not PUSH_PUBLIC, exist_ok=True)\n",
    "        except Exception: pass\n",
    "        try:\n",
    "            api.create_repo(ft_repo, repo_type=\"model\", private=not PUSH_PUBLIC, exist_ok=True)\n",
    "        except Exception: pass\n",
    "        try:\n",
    "            api.create_repo(delta_repo, repo_type=\"model\", private=not PUSH_PUBLIC, exist_ok=True)\n",
    "        except Exception: pass\n",
    "\n",
    "        upload_folder(folder_path=str(sft_path.parent), repo_id=pairs_repo, repo_type=\"dataset\", token=HF_TOKEN)\n",
    "        upload_folder(folder_path=st[\"models\"][\"nonthinking_ft\"], repo_id=ft_repo, repo_type=\"model\", token=HF_TOKEN)\n",
    "        upload_folder(folder_path=st[\"models\"][\"delta_model\"], repo_id=delta_repo, repo_type=\"model\", token=HF_TOKEN)\n",
    "        # also upload preds\n",
    "        preds_repo = f\"{HF_USERNAME}/{run_name}-cycle{cycle:03d}-preds\"\n",
    "        try:\n",
    "            api.create_repo(preds_repo, repo_type=\"dataset\", private=not PUSH_PUBLIC, exist_ok=True)\n",
    "        except Exception: pass\n",
    "        up_dir = run_dir / f\"cycle_{cycle:03d}\" / \"preds_bundle\"\n",
    "        up_dir.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(preds_json, up_dir / \"thinking_preds.json\")\n",
    "        shutil.copy2(preds2_json, up_dir / \"delta_preds.json\")\n",
    "        upload_folder(folder_path=str(up_dir), repo_id=preds_repo, repo_type=\"dataset\", token=HF_TOKEN)\n",
    "\n",
    "        st[\"paths\"][\"repos\"] = {\n",
    "            \"sft\": pairs_repo,\n",
    "            \"nonthink_ft\": ft_repo,\n",
    "            \"delta\": delta_repo,\n",
    "            \"preds\": preds_repo,\n",
    "        }\n",
    "        st[\"steps\"][\"uploads_done\"] = True\n",
    "        save_state(st, state_path, run_dir)\n",
    "\n",
    "    # 6) Optional: submit both to sb-cli on verified split using only mini IDs\n",
    "    try:\n",
    "        thinking_submit = sbcli_submit(predictions_path=preds_json, subset=\"swe-bench_verified\", split=\"dev\", run_id=f\"{run_name}-cycle{cycle:03d}-think\", instance_ids=st[\"instance_ids\"])\n",
    "        delta_submit = sbcli_submit(predictions_path=preds2_json, subset=\"swe-bench_verified\", split=\"dev\", run_id=f\"{run_name}-cycle{cycle:03d}-delta\", instance_ids=st[\"instance_ids\"])\n",
    "        st[\"reports\"][\"sbcli_thinking\"] = str(thinking_submit)[:4000]\n",
    "        st[\"reports\"][\"sbcli_delta\"] = str(delta_submit)[:4000]\n",
    "    except Exception as e:\n",
    "        st[\"reports\"][\"sbcli_error\"] = f\"{type(e).__name__}: {e}\"\n",
    "    save_state(st, state_path, run_dir)\n",
    "\n",
    "    # Prepare for next cycle if you rerun\n",
    "    st[\"cycle\"] = cycle + 1\n",
    "    # Keep bases fixed, but advance pointers\n",
    "    st[\"models\"][\"nonthinking_base\"] = st[\"models\"][\"nonthinking_base\"]\n",
    "    st[\"models\"][\"thinking_base\"] = st[\"models\"][\"thinking_base\"]\n",
    "    # Next cycle will start from:\n",
    "    st[\"models\"][\"nonthinking_for_cycle\"] = st[\"models\"][\"nonthinking_ft\"]\n",
    "    st[\"models\"][\"thinking_for_cycle\"] = st[\"models\"][\"delta_model\"]\n",
    "    # Reset step flags for the next call\n",
    "    st[\"steps\"] = {k: False for k in st[\"steps\"]}\n",
    "    save_state(st, state_path, run_dir)\n",
    "\n",
    "    return run_dir, st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae1444",
   "metadata": {},
   "source": [
    "\n",
    "## How to run\n",
    "\n",
    "1. Runtime type A100 80GB.\n",
    "2. Run Setup.\n",
    "3. Run `run_one_cycle_or_resume()` once. If Colab disconnects, just run it again - it resumes from Hub state. If it finishes fully, running again starts the next cycle and uses the latest weights for both models.\n",
    "4. Artifacts are in your Hub, state is mirrored to `dataset://oof-baroomf/a3b-mini-state/state/<run_id>/`.\n",
    "\n",
    "Notes\n",
    "\n",
    "- Agentic harness is **mini-swe-agent** LocalEnvironment, not direct prompting.\n",
    "- vLLM serves one model at a time. The notebook sequentially serves thinking, then the new model.\n",
    "- LoRA training uses Unsloth `FastModel` for MoE, rank-1, merge-every-step by callback.\n",
    "- No GGUF is used in training or merging.\n",
    "- Qwen best practices are enforced in the agent config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff57cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Run one cycle or resume the current one\n",
    "run_dir, state = run_one_cycle_or_resume(run_name=None)\n",
    "print(\"Run dir:\", run_dir)\n",
    "print(\"Latest state keys:\", list(state.keys()))\n",
    "print(\"Next cycle:\", state[\"cycle\"])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
