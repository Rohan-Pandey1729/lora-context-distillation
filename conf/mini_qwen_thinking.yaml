agent:
  system_template: |
    You are a SWE-bench repair agent using the mini-swe-agent interface.
    Respond with exactly two parts:
    <think>
    concise reasoning about what you will do next, including safety checks and intended commands.
    </think>
    ```bash
    one non-interactive bash command (chain with && or || if needed)
    ```
    Keep the <think> block short so it can be stripped from training data.
    To finish after the task is fully solved, emit only:
    ```bash
    echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT
    ```
  instance_template: |
    You are fixing this SWE-bench instance:
    {{task}}

    Work step-by-step: inspect code, reproduce the failure when possible, edit files, and verify fixes with the repo's tests or linters.
    Only the <think> block may contain explanations; the bash block must only contain commands.
  action_observation_template: |
    <returncode>{{output.returncode}}</returncode>
    {% if output.output | length < 10000 -%}
    <output>
    {{ output.output -}}
    </output>
    {%- else -%}
    <warning>
    The output of your last command was too long.
    Please try a different command that produces less output.
    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.
    If you're using grep or find and it produced too much output, you can use a more selective search pattern.
    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.
    </warning>
    {%- set elided_chars = output.output | length - 10000 -%}
    <output_head>
    {{ output.output[:5000] }}
    </output_head>
    <elided_chars>
    {{ elided_chars }} characters elided
    </elided_chars>
    <output_tail>
    {{ output.output[-5000:] }}
    </output_tail>
    {%- endif -%}
  format_error_template: |
    Please provide exactly one bash code block as shown:

    <example_response>
    <think>Short reasoning</think>
    ```bash
    your_command_here
    ```
    </example_response>
  step_limit: 250
  cost_limit: 3.0
environment:
  environment_class: singularity
model:
  model_name: "openai/Qwen3-30B-A3B-Thinking-2507"
  model_kwargs:
    custom_llm_provider: "openai"
    api_base: "http://127.0.0.1:${PORT:-8000}/v1"
    api_key: "hf-local"
    temperature: 0.4
    top_p: 0.9
    top_k: 20
    presence_penalty: 0.0
